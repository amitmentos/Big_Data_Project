# processing/spark-apps/bronze_ingestion.py

import argparse
import logging
from datetime import datetime, timedelta
from decimal import Decimal
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BronzeIngestion:
    def __init__(self):
        """Initialize Spark session with Iceberg and Kafka support"""
        self.spark = SparkSession.builder \
            .appName("Bronze Layer Ingestion") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog") \
            .config("spark.sql.catalog.spark_catalog.type", "hadoop") \
            .config("spark.sql.catalog.spark_catalog.warehouse", "s3a://warehouse/") \
            .config("spark.sql.catalog.spark_catalog.s3.endpoint", "http://minio:9000") \
            .config("spark.hadoop.fs.s3a.access.key", "minio") \
            .config("spark.hadoop.fs.s3a.secret.key", "minio123") \
            .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.path.style.access", "true") \
            .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
            .config("spark.hadoop.fs.s3a.attempts.maximum", "1") \
            .config("spark.hadoop.fs.s3a.connection.establish.timeout", "5000") \
            .config("spark.hadoop.fs.s3a.connection.timeout", "10000") \
            .config("spark.sql.streaming.checkpointLocation", "s3a://warehouse/checkpoints/") \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("WARN")

    def create_bronze_schemas(self):
        """Create Bronze layer database and tables if they don't exist"""
        
        # Create bronze database
        self.spark.sql("CREATE DATABASE IF NOT EXISTS bronze")
        
        # Raw User Events table (from Kafka stream)
        self.spark.sql("""
            CREATE TABLE IF NOT EXISTS bronze.raw_user_events (
                event_id STRING,
                session_id STRING,
                customer_id STRING,
                event_type STRING,
                event_time TIMESTAMP,
                page_url STRING,
                product_id STRING,
                device_type STRING,
                metadata MAP<STRING, STRING>,
                ingestion_time TIMESTAMP,
                kafka_timestamp TIMESTAMP,
                kafka_partition INT,
                kafka_offset BIGINT
            ) USING ICEBERG
            PARTITIONED BY (days(event_time))
            TBLPROPERTIES (
                'write.format.default' = 'parquet',
                'write.parquet.compression-codec' = 'snappy'
            )
        """)
        
        # Raw Marketplace Sales table (from Kafka stream with late arrivals)
        self.spark.sql("""
            CREATE TABLE IF NOT EXISTS bronze.raw_marketplace_sales (
                transaction_id STRING,
                marketplace_name STRING,
                seller_id STRING,
                product_id STRING,
                amount DECIMAL(10,2),
                currency STRING,
                quantity INT,
                transaction_time TIMESTAMP,
                settlement_time TIMESTAMP,
                payment_method STRING,
                status STRING,
                marketplace_metadata MAP<STRING, STRING>,
                ingestion_time TIMESTAMP,
                kafka_timestamp TIMESTAMP,
                kafka_partition INT,
                kafka_offset BIGINT
            ) USING ICEBERG
            PARTITIONED BY (days(transaction_time))
            TBLPROPERTIES (
                'write.format.default' = 'parquet',
                'write.parquet.compression-codec' = 'snappy'
            )
        """)
        
        # Raw Product Catalog table (batch data)
        self.spark.sql("""
            CREATE TABLE IF NOT EXISTS bronze.raw_product_catalog (
                product_id STRING,
                product_name STRING,
                category STRING,
                subcategory STRING,
                brand STRING,
                base_price DECIMAL(10,2),
                description STRING,
                is_active BOOLEAN,
                last_updated TIMESTAMP,
                ingestion_time TIMESTAMP
            ) USING ICEBERG
            PARTITIONED BY (days(last_updated))
            TBLPROPERTIES (
                'write.format.default' = 'parquet',
                'write.parquet.compression-codec' = 'snappy'
            )
        """)
        
        # Raw Customer Data table (batch data)
        self.spark.sql("""
            CREATE TABLE IF NOT EXISTS bronze.raw_customer_data (
                customer_id STRING,
                customer_name STRING,
                email STRING,
                address STRING,
                city STRING,
                country STRING,
                membership_tier STRING,
                created_at TIMESTAMP,
                last_updated TIMESTAMP,
                ingestion_time TIMESTAMP
            ) USING ICEBERG
            PARTITIONED BY (days(last_updated))
            TBLPROPERTIES (
                'write.format.default' = 'parquet',
                'write.parquet.compression-codec' = 'snappy'
            )
        """)
        
        # Raw Marketing Campaigns table (batch data)
        self.spark.sql("""
            CREATE TABLE IF NOT EXISTS bronze.raw_marketing_campaigns (
                campaign_id STRING,
                campaign_name STRING,
                campaign_type STRING,
                start_date DATE,
                end_date DATE,
                budget DECIMAL(15,2),
                target_audience STRING,
                ingestion_time TIMESTAMP
            ) USING ICEBERG
            TBLPROPERTIES (
                'write.format.default' = 'parquet',
                'write.parquet.compression-codec' = 'snappy'
            )
        """)

    def ingest_user_events_stream(self):
        """Ingest real-time user events from Kafka"""
        logger.info("Starting user events stream ingestion...")
        
        try:
            # Read from Kafka stream
            kafka_df = self.spark \
                .readStream \
                .format("kafka") \
                .option("kafka.bootstrap.servers", "kafka:29092") \
                .option("subscribe", "user_activity_events") \
                .option("startingOffsets", "latest") \
                .option("failOnDataLoss", "false") \
                .load()
            
            # Parse JSON messages
            user_events_schema = StructType([
                StructField("event_id", StringType(), True),
                StructField("session_id", StringType(), True),
                StructField("customer_id", StringType(), True),
                StructField("event_type", StringType(), True),
                StructField("event_time", StringType(), True),
                StructField("page_url", StringType(), True),
                StructField("device_type", StringType(), True),
                StructField("metadata", MapType(StringType(), StringType()), True),
                StructField("ingestion_time", StringType(), True)
            ])
            
            parsed_df = kafka_df.select(
                from_json(col("value").cast("string"), user_events_schema).alias("data"),
                col("timestamp").alias("kafka_timestamp"),
                col("partition").alias("kafka_partition"),
                col("offset").alias("kafka_offset")
            ).select(
                col("data.event_id"),
                col("data.session_id"),
                col("data.customer_id"),
                col("data.event_type"),
                to_timestamp(col("data.event_time")).alias("event_time"),
                col("data.page_url"),
                when(col("data.metadata.product_id").isNotNull(), 
                     col("data.metadata.product_id")).alias("product_id"),
                col("data.device_type"),
                col("data.metadata"),
                to_timestamp(col("data.ingestion_time")).alias("ingestion_time"),
                col("kafka_timestamp"),
                col("kafka_partition"),
                col("kafka_offset")
            ).filter(
                col("event_id").isNotNull() &
                col("customer_id").isNotNull() &
                col("event_type").isNotNull()
            )
            
            # Write stream to Bronze table
            query = parsed_df.writeStream \
                .format("iceberg") \
                .outputMode("append") \
                .option("table", "bronze.raw_user_events") \
                .option("checkpointLocation", "s3a://warehouse/checkpoints/user_events") \
                .trigger(processingTime='30 seconds') \
                .start()
            
            logger.info("User events stream ingestion started")
            return query
            
        except Exception as e:
            logger.error(f"Error starting user events stream: {str(e)}")
            raise

    def ingest_marketplace_sales_stream(self):
        """Ingest marketplace sales from Kafka (with late arrivals)"""
        logger.info("Starting marketplace sales stream ingestion...")
        
        try:
            # Read from Kafka stream
            kafka_df = self.spark \
                .readStream \
                .format("kafka") \
                .option("kafka.bootstrap.servers", "kafka:29092") \
                .option("subscribe", "marketplace_sales") \
                .option("startingOffsets", "latest") \
                .option("failOnDataLoss", "false") \
                .load()
            
            # Parse JSON messages
            sales_schema = StructType([
                StructField("transaction_id", StringType(), True),
                StructField("marketplace_name", StringType(), True),
                StructField("seller_id", StringType(), True),
                StructField("product_id", StringType(), True),
                StructField("amount", DecimalType(10,2), True),
                StructField("currency", StringType(), True),
                StructField("quantity", IntegerType(), True),
                StructField("transaction_time", StringType(), True),
                StructField("settlement_time", StringType(), True),
                StructField("payment_method", StringType(), True),
                StructField("status", StringType(), True),
                StructField("marketplace_metadata", MapType(StringType(), StringType()), True),
                StructField("ingestion_time", StringType(), True)
            ])
            
            parsed_df = kafka_df.select(
                from_json(col("value").cast("string"), sales_schema).alias("data"),
                col("timestamp").alias("kafka_timestamp"),
                col("partition").alias("kafka_partition"),
                col("offset").alias("kafka_offset")
            ).select(
                col("data.transaction_id"),
                col("data.marketplace_name"),
                col("data.seller_id"),
                col("data.product_id"),
                col("data.amount"),
                col("data.currency"),
                col("data.quantity"),
                to_timestamp(col("data.transaction_time")).alias("transaction_time"),
                to_timestamp(col("data.settlement_time")).alias("settlement_time"),
                col("data.payment_method"),
                col("data.status"),
                col("data.marketplace_metadata"),
                to_timestamp(col("data.ingestion_time")).alias("ingestion_time"),
                col("kafka_timestamp"),
                col("kafka_partition"),
                col("kafka_offset")
            ).filter(
                col("transaction_id").isNotNull() &
                col("product_id").isNotNull() &
                col("amount").isNotNull()
            )
            
            # Write stream to Bronze table
            query = parsed_df.writeStream \
                .format("iceberg") \
                .outputMode("append") \
                .option("table", "bronze.raw_marketplace_sales") \
                .option("checkpointLocation", "s3a://warehouse/checkpoints/marketplace_sales") \
                .trigger(processingTime='60 seconds') \
                .start()
            
            logger.info("Marketplace sales stream ingestion started")
            return query
            
        except Exception as e:
            logger.error(f"Error starting marketplace sales stream: {str(e)}")
            raise

    def ingest_batch_data(self, data_source: str, process_date: str):
        """Ingest batch data sources"""
        logger.info(f"Ingesting batch data: {data_source} for {process_date}")
        
        if data_source == "product_catalog":
            self.ingest_product_catalog(process_date)
        elif data_source == "customer_data":
            self.ingest_customer_data(process_date)
        elif data_source == "marketing_campaigns":
            self.ingest_marketing_campaigns(process_date)
        elif data_source == "sales_transactions":
            self.ingest_sales_transactions(process_date)
        elif data_source == "user_activity":
            self.ingest_user_activity(process_date)
        else:
            logger.warning(f"Unknown batch data source: {data_source}")

    def ingest_product_catalog(self, process_date: str):
        """Simulate product catalog ingestion"""
        logger.info("Generating sample product catalog data...")
        
        try:
            # Create dynamic sample data using timestamp-based IDs
            import random
            import hashlib
            
            # Use process_date and current timestamp to create unique data
            random.seed(hashlib.md5(f"{process_date}_{datetime.now().timestamp()}".encode()).hexdigest())
            
            data = []
            categories = ['Electronics', 'Clothing', 'Home & Garden', 'Sports', 'Books', 'Toys', 'Beauty', 'Automotive']
            subcategories = ['Smartphones', 'Laptops', 'Headphones', 'Cameras', 'Accessories']
            brands = ['TechBrand', 'StyleCorp', 'HomeMax', 'SportsPro', 'BookWorld', 'ToyLand', 'BeautyPlus', 'AutoTech']
            
            # Generate unique product ID base using timestamp
            timestamp_base = int(datetime.now().timestamp()) % 100000
            
            for i in range(1, 11):  # 10 products for testing
                product_id = f"PROD-{timestamp_base + i:06d}"
                category = categories[i % len(categories)]
                subcategory = subcategories[i % len(subcategories)]
                brand = brands[i % len(brands)]
                price_val = 10.50 + (i * 5.25)  # Simple price calculation
                
                data.append((
                    product_id,
                    f"Product {product_id}",
                    category,
                    subcategory,
                    brand,
                    f"{price_val:.2f}",  # Convert to string format
                    f"Description for {product_id} - {category} product",
                    True,
                    process_date,
                    datetime.now().isoformat()
                ))
            
            # Define schema explicitly
            schema = StructType([
                StructField("product_id", StringType(), True),
                StructField("product_name", StringType(), True),
                StructField("category", StringType(), True),
                StructField("subcategory", StringType(), True),
                StructField("brand", StringType(), True),
                StructField("base_price", StringType(), True),
                StructField("description", StringType(), True),
                StructField("is_active", BooleanType(), True),
                StructField("last_updated", StringType(), True),
                StructField("ingestion_time", StringType(), True)
            ])
            
            # Create DataFrame
            products_df = self.spark.createDataFrame(data, schema)
            record_count = len(data)  # Store count before DataFrame operations
            
            # Convert types
            products_df = products_df.select(
                col("product_id"),
                col("product_name"),
                col("category"),
                col("subcategory"),
                col("brand"),
                col("base_price").cast("decimal(10,2)").alias("base_price"),
                col("description"),
                col("is_active"),
                to_timestamp(col("last_updated")).alias("last_updated"),
                to_timestamp(col("ingestion_time")).alias("ingestion_time")
            )
            
            products_df.write \
                .mode("append") \
                .insertInto("bronze.raw_product_catalog")
            
            logger.info(f"Ingested {record_count} product records")
            
        except Exception as e:
            logger.error(f"Error ingesting product catalog: {str(e)}")
            raise

    def ingest_customer_data(self, process_date: str):
        """Simulate customer data ingestion"""
        logger.info("Generating sample customer data...")
        
        try:
            # Create dynamic sample data that changes each run
            import random
            import hashlib
            
            # Use process_date and current timestamp to create unique data
            random.seed(hashlib.md5(f"{process_date}_{datetime.now().timestamp()}".encode()).hexdigest())
            
            data = []
            cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Seattle', 'Miami', 'Boston', 'Denver', 'Austin']
            countries = ['USA', 'Canada', 'UK', 'Germany', 'France', 'Spain', 'Italy', 'Netherlands', 'Sweden', 'Australia']
            tiers = ['Bronze', 'Silver', 'Gold', 'Platinum']
            
            # Generate unique customer ID base using timestamp
            timestamp_base = int(datetime.now().timestamp()) % 100000
            
            for i in range(1, 101):  # 100 customers for testing
                customer_id = f"CUST-{timestamp_base + i:06d}"
                
                # Generate some variation in the data
                city_index = random.randint(0, len(cities) - 1)
                country_index = random.randint(0, len(countries) - 1)
                tier_index = random.randint(0, len(tiers) - 1)
                
                # Add some customers with updated info (for SCD testing)
                if i % 10 == 0:  # Every 10th customer gets updated info
                    membership_tier = tiers[(tier_index + 1) % len(tiers)]  # Promote tier
                    last_updated = process_date
                else:
                    membership_tier = tiers[tier_index]
                    last_updated = process_date
                
                data.append((
                    customer_id,
                    f"Customer {timestamp_base + i}",
                    f"customer{timestamp_base + i}@email.com",
                    f"{random.randint(1, 9999)} {random.choice(['Main', 'Oak', 'Pine', 'Elm', 'Park'])} St",
                    cities[city_index],
                    countries[country_index],
                    membership_tier,
                    "2024-01-01 00:00:00",
                    last_updated,
                    datetime.now().isoformat()
                ))
            
            # Define schema
            schema = StructType([
                StructField("customer_id", StringType(), True),
                StructField("customer_name", StringType(), True),
                StructField("email", StringType(), True),
                StructField("address", StringType(), True),
                StructField("city", StringType(), True),
                StructField("country", StringType(), True),
                StructField("membership_tier", StringType(), True),
                StructField("created_at", StringType(), True),
                StructField("last_updated", StringType(), True),
                StructField("ingestion_time", StringType(), True)
            ])
            
            # Create DataFrame
            customers_df = self.spark.createDataFrame(data, schema)
            record_count = len(data)  # Store count before DataFrame operations
            
            # Convert types
            customers_df = customers_df.select(
                col("customer_id"),
                col("customer_name"),
                col("email"),
                col("address"),
                col("city"),
                col("country"),
                col("membership_tier"),
                to_timestamp(col("created_at")).alias("created_at"),
                to_timestamp(col("last_updated")).alias("last_updated"),
                to_timestamp(col("ingestion_time")).alias("ingestion_time")
            )
            
            customers_df.write \
                .mode("append") \
                .insertInto("bronze.raw_customer_data")
            
            logger.info(f"Ingested {record_count} customer records")
            
        except Exception as e:
            logger.error(f"Error ingesting customer data: {str(e)}")
            raise

    def ingest_marketing_campaigns(self, process_date: str):
        """Simulate marketing campaigns ingestion"""
        logger.info("Generating sample marketing campaigns...")
        
        try:
            # Create simple sample data
            data = [
                (
                    'CAMP_001',
                    'Summer Sale 2024',
                    'Seasonal',
                    '2024-06-01',
                    '2024-08-31',
                    '50000.00',
                    'All Customers',
                    datetime.now().isoformat()
                ),
                (
                    'CAMP_002',
                    'New Customer Welcome',
                    'Onboarding',
                    '2024-01-01',
                    '2024-12-31',
                    '25000.00',
                    'New Customers',
                    datetime.now().isoformat()
                ),
                (
                    'CAMP_003',
                    'Black Friday 2024',
                    'Holiday',
                    '2024-11-25',
                    '2024-11-29',
                    '100000.00',
                    'High Value Customers',
                    datetime.now().isoformat()
                )
            ]
            
            # Define schema
            schema = StructType([
                StructField("campaign_id", StringType(), True),
                StructField("campaign_name", StringType(), True),
                StructField("campaign_type", StringType(), True),
                StructField("start_date", StringType(), True),
                StructField("end_date", StringType(), True),
                StructField("budget", StringType(), True),
                StructField("target_audience", StringType(), True),
                StructField("ingestion_time", StringType(), True)
            ])
            
            # Create DataFrame
            campaigns_df = self.spark.createDataFrame(data, schema)
            record_count = len(data)  # Store count before DataFrame operations
            
            # Convert types
            campaigns_df = campaigns_df.select(
                col("campaign_id"),
                col("campaign_name"),
                col("campaign_type"),
                to_date(col("start_date")).alias("start_date"),
                to_date(col("end_date")).alias("end_date"),
                col("budget").cast("decimal(15,2)").alias("budget"),
                col("target_audience"),
                to_timestamp(col("ingestion_time")).alias("ingestion_time")
            )
            
            campaigns_df.write \
                .mode("append") \
                .insertInto("bronze.raw_marketing_campaigns")
            
            logger.info(f"Ingested {record_count} campaign records")
            
        except Exception as e:
            logger.error(f"Error ingesting marketing campaigns: {str(e)}")
            raise

    def _generate_sales_data(self, process_date: str):
        """Generate sales transaction data outside of Spark context"""
        import random as py_random
        import hashlib
        from datetime import datetime, timedelta
        
        logger.info("DEBUG: Starting _generate_sales_data method...")
        
        # Use process_date and current timestamp to create unique data
        seed_value = hashlib.md5(f"{process_date}_{datetime.now().timestamp()}".encode()).hexdigest()
        py_random.seed(seed_value)
        logger.info(f"DEBUG: Set random seed to: {seed_value}")
        
        data = []
        marketplaces = ['Amazon', 'eBay', 'Shopify', 'Etsy', 'WooCommerce']
        payment_methods = ['Credit Card', 'PayPal', 'Bank Transfer', 'Digital Wallet', 'Cash']
        currencies = ['USD', 'EUR', 'GBP', 'CAD', 'AUD']
        statuses = ['completed', 'pending', 'shipped', 'delivered']
        
        # Generate unique transaction ID base using timestamp
        timestamp_base = int(datetime.now().timestamp()) % 100000
        logger.info(f"DEBUG: timestamp_base = {timestamp_base}")
        
        # Generate sales records
        for i in range(1, 51):  # Generate 50 records
            try:
                logger.info(f"DEBUG: Generating record {i}...")
                
                # Generate IDs
                transaction_id = f"TXN-{timestamp_base + i:08d}"
                customer_id = f"CUST-{timestamp_base + py_random.randint(1, 100):06d}"
                product_id = f"PROD-{timestamp_base + py_random.randint(1, 10):06d}"
                
                # Generate transaction details
                marketplace = py_random.choice(marketplaces)
                seller_id = f"SELLER-{py_random.randint(1000, 9999)}"
                
                # Generate amount as string from the start
                raw_amount = py_random.uniform(10.00, 500.00)
                amount_str = f"{raw_amount:.2f}"
                
                # Generate quantity as string from the start  
                raw_quantity = py_random.randint(1, 5)
                quantity_str = str(raw_quantity)
                
                currency = py_random.choice(currencies)
                payment_method = py_random.choice(payment_methods)
                status = py_random.choice(statuses)
                
                # Create transaction time around the process date
                base_time = datetime.strptime(process_date, '%Y-%m-%d')
                transaction_time = base_time + timedelta(
                    hours=py_random.randint(0, 23),
                    minutes=py_random.randint(0, 59),
                    seconds=py_random.randint(0, 59)
                )
                settlement_time = transaction_time + timedelta(hours=py_random.randint(1, 72))
                
                # Create metadata as dict
                metadata = {
                    'promotion_code': f"PROMO{py_random.randint(100, 999)}" if py_random.random() > 0.7 else "",
                    'device_type': py_random.choice(['mobile', 'desktop', 'tablet']),
                    'channel': py_random.choice(['web', 'mobile_app', 'api'])
                }
                
                # Create the record tuple
                record_tuple = (
                    transaction_id,
                    marketplace,
                    seller_id,
                    product_id,
                    amount_str,  # STRING
                    currency,
                    quantity_str,  # STRING
                    transaction_time.isoformat(),
                    settlement_time.isoformat(),
                    payment_method,
                    status,
                    metadata,
                    datetime.now().isoformat(),
                    datetime.now().isoformat(),
                    str(0),
                    str(i)
                )
                
                data.append(record_tuple)
                
                if i <= 5:  # Log first 5 records in detail
                    logger.info(f"DEBUG: Record {i} created successfully: amount={amount_str}, quantity={quantity_str}")
                
            except Exception as record_error:
                logger.error(f"DEBUG: Error generating record {i}: {record_error}")
                raise
        
        logger.info(f"DEBUG: Generated {len(data)} total records")
        return data

    def ingest_sales_transactions(self, process_date: str):
        """Simulate sales transactions ingestion"""
        logger.info("Generating sample sales transactions...")
        
        try:
            # Use the separate data generation method that works outside Spark context
            logger.info("DEBUG: Generating dynamic sales data...")
            
            data = self._generate_sales_data(process_date)
            
            logger.info(f"DEBUG: Generated {len(data)} dynamic records")
            logger.info(f"DEBUG: About to create schema with {len(data)} records")
            
            # Define schema with STRING types for amount and quantity initially
            schema = StructType([
                StructField("transaction_id", StringType(), True),
                StructField("marketplace_name", StringType(), True),
                StructField("seller_id", StringType(), True),
                StructField("product_id", StringType(), True),
                StructField("amount", StringType(), True),  # Start as STRING
                StructField("currency", StringType(), True),
                StructField("quantity", StringType(), True),  # Start as STRING
                StructField("transaction_time", StringType(), True),
                StructField("settlement_time", StringType(), True),
                StructField("payment_method", StringType(), True),
                StructField("status", StringType(), True),
                StructField("marketplace_metadata", MapType(StringType(), StringType()), True),
                StructField("ingestion_time", StringType(), True),
                StructField("kafka_timestamp", StringType(), True),
                StructField("kafka_partition", StringType(), True),
                StructField("kafka_offset", StringType(), True)
            ])
            
            logger.info("DEBUG: Schema defined successfully")
            
            # Create DataFrame with detailed error handling
            try:
                logger.info("DEBUG: About to create DataFrame...")
                sales_df = self.spark.createDataFrame(data, schema)
                logger.info("DEBUG: DataFrame created successfully")
                record_count = len(data)  # Store count before DataFrame operations
                logger.info(f"DEBUG: Record count: {record_count}")
            except Exception as df_error:
                logger.error(f"DEBUG: Error creating DataFrame: {str(df_error)}")
                logger.error(f"DEBUG: Error type: {type(df_error)}")
                logger.error(f"DEBUG: Data sample: {data[:2] if data else 'No data'}")
                raise
            
            # Convert types with detailed error handling
            try:
                logger.info("DEBUG: About to convert types...")
                sales_df = sales_df.select(
                    col("transaction_id"),
                    col("marketplace_name"),
                    col("seller_id"),
                    col("product_id"),
                    col("amount").cast("decimal(10,2)").alias("amount"),  # Cast from STRING to DECIMAL
                    col("currency"),
                    col("quantity").cast("int").alias("quantity"),  # Cast from STRING to INT
                    to_timestamp(col("transaction_time")).alias("transaction_time"),
                    to_timestamp(col("settlement_time")).alias("settlement_time"),
                    col("payment_method"),
                    col("status"),
                    col("marketplace_metadata"),  # Already MAP type
                    to_timestamp(col("ingestion_time")).alias("ingestion_time"),
                    to_timestamp(col("kafka_timestamp")).alias("kafka_timestamp"),
                    col("kafka_partition").cast("int").alias("kafka_partition"),
                    col("kafka_offset").cast("bigint").alias("kafka_offset")
                )
                logger.info("DEBUG: Type conversion completed successfully")
            except Exception as conv_error:
                logger.error(f"DEBUG: Error during type conversion: {str(conv_error)}")
                logger.error(f"DEBUG: Conversion error type: {type(conv_error)}")
                raise
            
            sales_df.write \
                .mode("append") \
                .insertInto("bronze.raw_marketplace_sales")
            
            logger.info(f"Ingested {record_count} sales transaction records")
            
        except Exception as e:
            logger.error(f"Error ingesting sales transactions: {str(e)}")
            raise

    def ingest_user_activity(self, process_date: str):
        """Simulate user activity events ingestion"""
        logger.info("Generating sample user activity events...")
        
        try:
            # Use static data to test schema/DataFrame creation
            logger.info("DEBUG: Creating static test data for user activity...")
            
            data = [
                (
                    "EVT-00000001",
                    "SES-00000001",
                    "CUST-000001",
                    "page_view",
                    "2025-06-21T10:00:00",
                    "/home",
                    None,
                    "mobile",
                    {"user_agent": "Browser/123", "ip_address": "192.168.1.1", "referrer": "google.com"},
                    "2025-06-21T14:00:00",
                    "2025-06-21T14:00:00",
                    "0",
                    "1"
                ),
                (
                    "EVT-00000002",
                    "SES-00000001",
                    "CUST-000001",
                    "product_view",
                    "2025-06-21T10:05:00",
                    "/product/detail",
                    "PROD-000001",
                    "mobile",
                    {"user_agent": "Browser/123", "ip_address": "192.168.1.1", "referrer": "google.com", "product_id": "PROD-000001"},
                    "2025-06-21T14:00:00",
                    "2025-06-21T14:00:00",
                    "0",
                    "2"
                ),
                (
                    "EVT-00000003",
                    "SES-00000002",
                    "CUST-000002",
                    "add_to_cart",
                    "2025-06-21T11:30:00",
                    "/cart",
                    "PROD-000002",
                    "desktop",
                    {"user_agent": "Browser/456", "ip_address": "192.168.1.2", "referrer": "facebook.com", "product_id": "PROD-000002"},
                    "2025-06-21T14:00:00",
                    "2025-06-21T14:00:00",
                    "0",
                    "3"
                )
            ]
            
            logger.info(f"DEBUG: Created {len(data)} static user activity records")
            
            logger.info("DEBUG: About to create user activity schema...")
            
            # Define schema with MAP type for metadata
            schema = StructType([
                StructField("event_id", StringType(), True),
                StructField("session_id", StringType(), True),
                StructField("customer_id", StringType(), True),
                StructField("event_type", StringType(), True),
                StructField("event_time", StringType(), True),
                StructField("page_url", StringType(), True),
                StructField("product_id", StringType(), True),
                StructField("device_type", StringType(), True),
                StructField("metadata", MapType(StringType(), StringType()), True),
                StructField("ingestion_time", StringType(), True),
                StructField("kafka_timestamp", StringType(), True),
                StructField("kafka_partition", StringType(), True),
                StructField("kafka_offset", StringType(), True)
            ])
            
            logger.info("DEBUG: User activity schema defined successfully")
            
            # Create DataFrame with detailed error handling
            try:
                logger.info("DEBUG: About to create user activity DataFrame...")
                events_df = self.spark.createDataFrame(data, schema)
                logger.info("DEBUG: User activity DataFrame created successfully")
                record_count = len(data)
                logger.info(f"DEBUG: User activity record count: {record_count}")
            except Exception as df_error:
                logger.error(f"DEBUG: Error creating user activity DataFrame: {str(df_error)}")
                logger.error(f"DEBUG: Error type: {type(df_error)}")
                logger.error(f"DEBUG: Data sample: {data[:2] if data else 'No data'}")
                raise
            
            
            # Convert timestamp types with detailed error handling
            try:
                logger.info("DEBUG: About to convert user activity types...")
                events_df = events_df.select(
                    col("event_id"),
                    col("session_id"),
                    col("customer_id"),
                    col("event_type"),
                    to_timestamp(col("event_time")).alias("event_time"),
                    col("page_url"),
                    col("product_id"),
                    col("device_type"),
                    col("metadata"),  # Already MAP type
                    to_timestamp(col("ingestion_time")).alias("ingestion_time"),
                    to_timestamp(col("kafka_timestamp")).alias("kafka_timestamp"),
                    col("kafka_partition").cast("int").alias("kafka_partition"),
                    col("kafka_offset").cast("bigint").alias("kafka_offset")
                )
                logger.info("DEBUG: User activity type conversion completed successfully")
            except Exception as conv_error:
                logger.error(f"DEBUG: Error during user activity type conversion: {str(conv_error)}")
                logger.error(f"DEBUG: Conversion error type: {type(conv_error)}")
                raise
            
            # Write to table
            logger.info("DEBUG: Writing user activity to Bronze table...")
            try:
                events_df.write \
                    .mode("append") \
                    .insertInto("bronze.raw_user_events")
                logger.info("DEBUG: Successfully wrote user activity to Bronze table")
            except Exception as write_error:
                logger.error(f"DEBUG: Error writing user activity to table: {write_error}")
                import traceback
                logger.error(f"DEBUG: Write traceback: {traceback.format_exc()}")
                raise
            
            logger.info(f"Ingested {record_count} user activity event records")
            
        except Exception as e:
            logger.error(f"Error ingesting user activity events: {str(e)}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            raise

def main():
    parser = argparse.ArgumentParser(description='Bronze Layer Ingestion')
    parser.add_argument('--mode', required=True, 
                       choices=['stream', 'batch'],
                       help='Ingestion mode: stream or batch')
    parser.add_argument('--source', 
                       choices=['user_events', 'marketplace_sales', 'product_catalog', 
                               'customer_data', 'marketing_campaigns', 'sales_transactions', 
                               'user_activity', 'all'],
                       help='Data source to ingest')
    parser.add_argument('--date', 
                       help='Processing date for batch ingestion (YYYY-MM-DD)')
    
    args = parser.parse_args()
    
    ingestion = BronzeIngestion()
    
    try:
        # Create schemas
        ingestion.create_bronze_schemas()
        
        if args.mode == 'stream':
            logger.info("Starting streaming ingestion...")
            
            queries = []
            
            if args.source in ['user_events', 'all']:
                queries.append(ingestion.ingest_user_events_stream())
            
            if args.source in ['marketplace_sales', 'all']:
                queries.append(ingestion.ingest_marketplace_sales_stream())
            
            if queries:
                # Wait for all streams to complete
                for query in queries:
                    query.awaitTermination()
            else:
                logger.warning("No streams to start")
                
        elif args.mode == 'batch':
            if not args.date:
                logger.error("Date parameter required for batch ingestion")
                sys.exit(1)
            
            if args.source == 'all':
                ingestion.ingest_batch_data('product_catalog', args.date)
                ingestion.ingest_batch_data('customer_data', args.date)
                ingestion.ingest_batch_data('marketing_campaigns', args.date)
                ingestion.ingest_batch_data('sales_transactions', args.date)
                ingestion.ingest_batch_data('user_activity', args.date)
            else:
                ingestion.ingest_batch_data(args.source, args.date)
        
        logger.info("Bronze ingestion completed successfully")
        
    except Exception as e:
        logger.error(f"Bronze ingestion failed: {str(e)}")
        sys.exit(1)
    finally:
        ingestion.spark.stop()

if __name__ == "__main__":
    main()